{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM e GRU con Keras.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM6b4VV9idqThmoA5is2LeG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zideric/colab/blob/main/LSTM_e_GRU_con_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tN7zuTp-5m-"
      },
      "source": [
        "# LSTM e GRU con Keras\n",
        "le reti long short-term memory sono delle reti ricorrenti che risolvono il problema della scomparsa del gradiente tra le diverse esecuzioni della rete.\n",
        "\n",
        "le LSTM in sostanza aggiungono un canale prioritario alla rete, chiamato cell state o memory cell, che viaggia in parallelo al segnale della rete e immagazzina le informazioni sequenziali\n",
        "\n",
        "Qui la [ricerca](https://www.bioinf.jku.at/publications/older/2604.pdf)\n",
        "\n",
        "usiamo LSTM per la sentiment analysis dell'IMDB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAy9R7te_W_F"
      },
      "source": [
        "from time import time\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kqo9rw68_iQW"
      },
      "source": [
        "## Scarichiamo il dataset\n",
        "come al solito con Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYM_2Ecj_hn-",
        "outputId": "8535ab3c-839a-4702-fc96-a52e27ea1504"
      },
      "source": [
        "from keras.datasets import imdb\n",
        "\n",
        "num_words = 10000\n",
        "\n",
        "(X_train, y_train),(X_test,y_test) = imdb.load_data(num_words = num_words)\n",
        "\n",
        "print(\"Numero di esempi nel train set: %d\" % len(X_train))\n",
        "print(\"Numero di esempi nel test set: %d\" % len(X_test))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Numero di esempi nel train set: 25000\n",
            "Numero di esempi nel test set: 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuPxbmXh_x59"
      },
      "source": [
        "## Processiamo i dati\n",
        "usiamo sempre pad_sequences di keras per limitare le sequenze a 500 elementi (in questo caso per limitare le frasi a 500 parole). se è piu corta verranno aggiunti degli 0 finali"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-2ipAK-_v9i",
        "outputId": "bf9f1fba-6c80-44e0-e162-7b58bad1c7b8"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "maxlen = 500\n",
        "\n",
        "X_train = pad_sequences(X_train, maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, maxlen=maxlen)\n",
        "\n",
        "X_train.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 500)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awW6GIMCAKHI"
      },
      "source": [
        "## Creiamo il modello LSTM\n",
        "il modelllo sarà cosi strutturato:\n",
        "* primo strato esegue embedding creando 50 embedding vectors per ognuna delle 10000 parole del nostro dizionario\n",
        "* il secondo è lo strato ricorrente di tipo LSTM\n",
        "* il terzo strato calcolerà l'output della rete, trattandosi di classificazione bianria (postivia/negativa) la funzione sarà la sigmoide"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOkguKP-AH5w",
        "outputId": "ef7aff1d-0e22-46f6-819d-ee211bbc1f75"
      },
      "source": [
        "from keras.layers import Embedding, LSTM\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(num_words, 50))\n",
        "model.add(LSTM(32))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 50)          500000    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 32)                10624     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 510,657\n",
            "Trainable params: 510,657\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQn9gRh8Av2k"
      },
      "source": [
        "compiliamo il modello ed eseguiamolo per 5 epoche"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gx87-yC0AsGK",
        "outputId": "428e533e-6b7d-42cb-e43c-378712026e0e"
      },
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train,batch_size=512, validation_split=0.2,epochs=5)\n",
        "model.evaluate(X_test,y_test)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "40/40 [==============================] - 49s 1s/step - loss: 0.6549 - accuracy: 0.6209 - val_loss: 0.4609 - val_accuracy: 0.8096\n",
            "Epoch 2/5\n",
            "40/40 [==============================] - 46s 1s/step - loss: 0.4031 - accuracy: 0.8431 - val_loss: 0.6534 - val_accuracy: 0.7142\n",
            "Epoch 3/5\n",
            "40/40 [==============================] - 46s 1s/step - loss: 0.3347 - accuracy: 0.8696 - val_loss: 0.6682 - val_accuracy: 0.7660\n",
            "Epoch 4/5\n",
            "40/40 [==============================] - 46s 1s/step - loss: 0.3114 - accuracy: 0.8842 - val_loss: 0.3285 - val_accuracy: 0.8638\n",
            "Epoch 5/5\n",
            "40/40 [==============================] - 46s 1s/step - loss: 0.2196 - accuracy: 0.9230 - val_loss: 0.2929 - val_accuracy: 0.8804\n",
            "782/782 [==============================] - 31s 40ms/step - loss: 0.3022 - accuracy: 0.8743\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3021528422832489, 0.8743199706077576]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2P39o1wBLL9"
      },
      "source": [
        "il risultato è nettamente migliore di un RNN, c'è un po di overfitting che proviamo a mitigare utilizzando il Dropout\n",
        "\n",
        "## Dropout di una RNN\n",
        "\n",
        "in una RNN possiamo utilizzare due approcci per il dropout (2 tipologie):\n",
        "* Dropout sull'input/output dello strato, esattamente come abbiamo già fatto con le altre architetture di reti neurali\n",
        "* Dropout tra le esecuzion ricorrenti della rete, questo permette di ridurre l'overfitting nelle features che contengono le informazioni sulla seuqenza.\n",
        "\n",
        "Per utilizzre il dropout sull'input di uno strato ricorrente, piuttosto che usare la classe Dropoiut, è consigliato sfruttare il parametro dropout delle classi SimpleRNN e LSTM. Per utilizzre il dropout ricorrente possiamo invece utilizzre il parametro recurrent_dropout delle classi SIMpleRNN e LSTM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvQSZAPfBAuO",
        "outputId": "0f88cd6e-27ca-452e-df9f-359b8855848b"
      },
      "source": [
        "from keras.layers import Embedding, LSTM, Dropout\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(num_words, 50))\n",
        "model.add(LSTM(32, dropout=0.4, recurrent_dropout=0.2))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, None, 50)          500000    \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 32)                10624     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 510,657\n",
            "Trainable params: 510,657\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1YRj6eFCfue"
      },
      "source": [
        "compiliamo e addestriamo per 5 epoche"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiWkh9dACdqJ",
        "outputId": "0e993435-9cbd-4ee1-cf33-2c818b0dc5c3"
      },
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, batch_size=512, validation_split=0.2, epochs=5)\n",
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "40/40 [==============================] - 92s 2s/step - loss: 0.6793 - accuracy: 0.5853 - val_loss: 0.6905 - val_accuracy: 0.5748\n",
            "Epoch 2/5\n",
            "40/40 [==============================] - 89s 2s/step - loss: 0.4947 - accuracy: 0.7937 - val_loss: 0.5358 - val_accuracy: 0.7568\n",
            "Epoch 3/5\n",
            "40/40 [==============================] - 89s 2s/step - loss: 0.3685 - accuracy: 0.8578 - val_loss: 0.4262 - val_accuracy: 0.8356\n",
            "Epoch 4/5\n",
            "40/40 [==============================] - 88s 2s/step - loss: 0.3116 - accuracy: 0.8901 - val_loss: 0.3282 - val_accuracy: 0.8722\n",
            "Epoch 5/5\n",
            "40/40 [==============================] - 88s 2s/step - loss: 0.2662 - accuracy: 0.9003 - val_loss: 0.4809 - val_accuracy: 0.8108\n",
            "782/782 [==============================] - 43s 55ms/step - loss: 0.4797 - accuracy: 0.8058\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.47971367835998535, 0.8058000206947327]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biSYudJKCxb9"
      },
      "source": [
        "l'overfitting è stato ridotto, proviamo a migliorare il modello aggiungendo altri strati ricrrenti alla rete.\n",
        "\n",
        "## Aggiungiamo altri strati LSTM\n",
        "\n",
        "possiamo aggingere altri strati ricorrenti nella solita maniera. di default la classe LSTM esegue il flattening della sequenza per poterla dare come input ad uno strato denso, possiamo modificare tale comportamento impostando il parametro return_sequences = True."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aTn1w8wCwjR",
        "outputId": "9f188ebd-19da-4b86-f81f-e282e01a8778"
      },
      "source": [
        "from keras.layers import Embedding, LSTM, Dropout\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(num_words, 100)) #prima abbiamo eseguito con 50 embedding vectors\n",
        "model.add(LSTM(32, dropout=0.5, recurrent_dropout=0.2, return_sequences=True))\n",
        "model.add(LSTM(32, dropout=0.5, recurrent_dropout=0.2))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, None, 100)         1000000   \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, None, 32)          17024     \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 32)                8320      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 1,025,377\n",
            "Trainable params: 1,025,377\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDCAJThQDy1V"
      },
      "source": [
        "compiliamo ed eseguiamo le 5 epoche. questa volta cronometriamo perche confronteremo questo risultato con la variante GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iI4X2rp4DyP5",
        "outputId": "ab989bff-a93a-44c4-9660-f02e8c8a2b8f"
      },
      "source": [
        "model.compile(loss='binary_crossentropy',optimizer='rmsprop', metrics=['accuracy'])\n",
        "start_at=time()\n",
        "model.fit(X_train, y_train, batch_size=512, validation_split=0.2, epochs=5)\n",
        "print(\"Addestramento completato in %.f secondi (5 epoche)\" % ((time()-start_at)))\n",
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "40/40 [==============================] - 203s 5s/step - loss: 0.6493 - accuracy: 0.6022 - val_loss: 0.3836 - val_accuracy: 0.8416\n",
            "Epoch 2/5\n",
            "40/40 [==============================] - 201s 5s/step - loss: 0.4084 - accuracy: 0.8226 - val_loss: 0.3211 - val_accuracy: 0.8644\n",
            "Epoch 3/5\n",
            "40/40 [==============================] - 202s 5s/step - loss: 0.3044 - accuracy: 0.8811 - val_loss: 0.3561 - val_accuracy: 0.8712\n",
            "Epoch 4/5\n",
            "40/40 [==============================] - 202s 5s/step - loss: 0.2961 - accuracy: 0.8871 - val_loss: 0.4549 - val_accuracy: 0.8558\n",
            "Epoch 5/5\n",
            "40/40 [==============================] - 202s 5s/step - loss: 0.2710 - accuracy: 0.9022 - val_loss: 0.3174 - val_accuracy: 0.8892\n",
            "Addestramento completato in 1010 secondi (5 epoche)\n",
            "782/782 [==============================] - 84s 107ms/step - loss: 0.3529 - accuracy: 0.8752\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3529222011566162, 0.8751999735832214]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaT6DxRtENXD"
      },
      "source": [
        "## Grated Recurrent UNIT (GRU)\n",
        "sono una tipologia di reti neurali ricorrenti che prendono spunto da e semplificano le LSTM. a differenza di quest'ultime le GRU richiedono meno calcoli tensoriali e quindi solitamente portano a risultati simili in minor tempo.\n",
        "Possiamo aggiungere gli strati GRU con Keras tramite la classe GRU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xoF8pSJ4ELf-",
        "outputId": "13f5f466-6f90-4db7-cd46-a1f3985eb3f5"
      },
      "source": [
        "from keras.layers import Embedding, GRU, Dropout\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(num_words, 100))\n",
        "model.add(GRU(32, dropout=0.5, recurrent_dropout=0.2, return_sequences=True))\n",
        "model.add(GRU(32, dropout=0.5, recurrent_dropout=0.2))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, None, 100)         1000000   \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (None, None, 32)          12864     \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (None, 32)                6336      \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 1,019,233\n",
            "Trainable params: 1,019,233\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ho7qaxX0FU1f"
      },
      "source": [
        "compiliamo e avviamo l'addestramento su 5 epoche cronometrando."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHbX33PoFTq1",
        "outputId": "2cc4baf6-b059-475d-970c-ebc2ba8fc69e"
      },
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop',metrics=['accuracy'])\n",
        "start_at=time()\n",
        "model.fit(X_train,y_train,batch_size=512,validation_split=0.2,epochs=5)\n",
        "print(\"Addestramento completato in %.f secondi (5 epoche)\" % ((time()-start_at)))\n",
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "40/40 [==============================] - 168s 4s/step - loss: 0.6721 - accuracy: 0.5699 - val_loss: 0.5175 - val_accuracy: 0.7428\n",
            "Epoch 2/5\n",
            "40/40 [==============================] - 166s 4s/step - loss: 0.4709 - accuracy: 0.7811 - val_loss: 0.4153 - val_accuracy: 0.8158\n",
            "Epoch 3/5\n",
            "40/40 [==============================] - 166s 4s/step - loss: 0.3754 - accuracy: 0.8451 - val_loss: 0.4130 - val_accuracy: 0.8188\n",
            "Epoch 4/5\n",
            "40/40 [==============================] - 165s 4s/step - loss: 0.3392 - accuracy: 0.8629 - val_loss: 0.3719 - val_accuracy: 0.8400\n",
            "Epoch 5/5\n",
            "40/40 [==============================] - 165s 4s/step - loss: 0.3050 - accuracy: 0.8765 - val_loss: 0.3672 - val_accuracy: 0.8588\n",
            "Addestramento completato in 831 secondi (5 epoche)\n",
            "782/782 [==============================] - 85s 109ms/step - loss: 0.3840 - accuracy: 0.8445\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.38397184014320374, 0.8444799780845642]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJFGCfi1FxbC"
      },
      "source": [
        "\n",
        "Il risultato è livemente inferiore a quello ottenuto con le LSTM ma l'addestramento ha richiesto il 20% del tempo in meno."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVOp6YNvFuV7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
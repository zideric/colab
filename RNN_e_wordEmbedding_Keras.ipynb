{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_e_wordEmbedding_Keras.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOjW2hLHDygLTeJe1N/ewx1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zideric/colab/blob/main/RNN_e_wordEmbedding_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9RNHR607PI2"
      },
      "source": [
        "# RNN e Word Embedding con Keras\n",
        "RNN sta per rete neurale ricorrente. sono un architettura di reti neurali che manenendo una memorria interna ci permettono di analizzare sequenze temporali di dati\n",
        "\n",
        "costruiamo una rete neurale ricorreten per un problema di sentiment analysis utilizzando IMDB database"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLECdOHY7TYQ"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9uUsb9m7eDJ"
      },
      "source": [
        "## Scarichiamo il dataset\n",
        "utilizziamo Keras per caricare imdb dataset limitandolo a 10000 parole piu comuni"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKYMQKZ_7bLu",
        "outputId": "f5cbf9a6-7297-4458-9620-c08e897edf05"
      },
      "source": [
        "from keras.datasets import imdb\n",
        "\n",
        "num_words = 10000\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=num_words)\n",
        "\n",
        "print(\"Numero di esempi nel train set: %d\" % len(X_train))\n",
        "print(\"Numero di esempi nel test set: %d\" % len(X_test))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Numero di esempi nel train set: 25000\n",
            "Numero di esempi nel test set: 25000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KG-YM0427zaM"
      },
      "source": [
        "## Processiamo i dati\n",
        "\n",
        "per rappresentare le sequenze utilizzeremo il Word Embedding che va agggiunto come strato iniziale della rete neurale. le recensioni all'interno del corpus ovviamente possono avere una lunghezza differente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRXF-gI_7yGe",
        "outputId": "3fe0a2ab-4c84-4adb-b21a-3a83b5f5b845"
      },
      "source": [
        "longest_review = max(X_train, key=len) #applichiamo funzione max al set di day e specifichiamo che la chiave per cui vogliamo il max è len\n",
        "shortest_review = min(X_train,key=len)\n",
        "\n",
        "print(\"La review più lunga ha %d parole\" % len(longest_review))\n",
        "print(\"La reveiw più corta ha %d parole\" % len(shortest_review))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "La review più lunga ha 2494 parole\n",
            "La reveiw più corta ha 11 parole\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_LgGBVb8dDY"
      },
      "source": [
        "tronchiamole dopo 100 parole utilizzando la funzione pad_sequence di keras. Se una recensione ha menod di 100 parole verranno aggiunti una serie di zeri al termine per portarlo alla lungezza corretta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nI6R7tGW8aJB",
        "outputId": "a6ef7289-703c-4a21-f3d8-ac614bb293ae"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "maxlen = 100\n",
        "\n",
        "X_train = pad_sequences(X_train, maxlen= maxlen)\n",
        "X_test = pad_sequences(X_test, maxlen=maxlen)\n",
        "\n",
        "X_train.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9OjmQ5u84ZE"
      },
      "source": [
        "## Creiamo il modello\n",
        "\n",
        "il primo strato del modello eseguirà il Word Embedding, è un modello che ci permette di generare una serie di vettori ognuno dei quali quantifica una caratteristica delle parole. Questo tipo di rappresentazione che tiene conto di relazion e dipendenze tra le parole è un ottimo input per una RNN\n",
        "\n",
        "La classe Embedding di Keras ha due parametri:\n",
        "* il numero di parole nel nostro dizionario\n",
        "* il numero di embedding vectors da generare\n",
        "\n",
        "per aggiungere lo strato ricorrente alla nostra rete possiamo usare la classe SimpleRNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01sULgqW82lR",
        "outputId": "c5e9cd2d-8165-465e-bb1d-e64bd31d4ee7"
      },
      "source": [
        "from keras.layers import Embedding, SimpleRNN\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(num_words, 50)) #50 è il numero di embedding vectors\n",
        "model.add(SimpleRNN(32))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 50)          500000    \n",
            "_________________________________________________________________\n",
            "simple_rnn (SimpleRNN)       (None, 32)                2656      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 502,689\n",
            "Trainable params: 502,689\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6OPoY_H9rK6"
      },
      "source": [
        "compiliamo il modello ed eseguiamo l'addestramento su sole 10 epoche.\n",
        "come algoritmo di ottimizzazione per RNN Keras consiglia di usare ** RMSprop ** anche se non ci sono evidenze sperimentali."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95Peg_-x9m5j",
        "outputId": "0ea8b956-1147-458a-bb97-1219ff31f06d"
      },
      "source": [
        "model.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, batch_size=512, validation_split=0.2, epochs=10)\n",
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "40/40 [==============================] - 5s 94ms/step - loss: 0.6869 - accuracy: 0.5353 - val_loss: 0.6063 - val_accuracy: 0.6582\n",
            "Epoch 2/10\n",
            "40/40 [==============================] - 3s 85ms/step - loss: 0.5225 - accuracy: 0.7639 - val_loss: 0.5010 - val_accuracy: 0.7576\n",
            "Epoch 3/10\n",
            "40/40 [==============================] - 3s 85ms/step - loss: 0.3701 - accuracy: 0.8501 - val_loss: 0.5365 - val_accuracy: 0.7712\n",
            "Epoch 4/10\n",
            "40/40 [==============================] - 3s 85ms/step - loss: 0.3003 - accuracy: 0.8862 - val_loss: 0.4369 - val_accuracy: 0.8214\n",
            "Epoch 5/10\n",
            "40/40 [==============================] - 3s 84ms/step - loss: 0.2266 - accuracy: 0.9197 - val_loss: 0.4111 - val_accuracy: 0.8272\n",
            "Epoch 6/10\n",
            "40/40 [==============================] - 3s 84ms/step - loss: 0.1748 - accuracy: 0.9417 - val_loss: 0.3957 - val_accuracy: 0.8362\n",
            "Epoch 7/10\n",
            "40/40 [==============================] - 3s 83ms/step - loss: 0.1254 - accuracy: 0.9630 - val_loss: 0.5219 - val_accuracy: 0.7600\n",
            "Epoch 8/10\n",
            "40/40 [==============================] - 3s 85ms/step - loss: 0.0997 - accuracy: 0.9724 - val_loss: 0.6132 - val_accuracy: 0.7214\n",
            "Epoch 9/10\n",
            "40/40 [==============================] - 3s 84ms/step - loss: 0.0799 - accuracy: 0.9793 - val_loss: 0.5316 - val_accuracy: 0.7804\n",
            "Epoch 10/10\n",
            "40/40 [==============================] - 3s 84ms/step - loss: 0.0460 - accuracy: 0.9892 - val_loss: 0.5945 - val_accuracy: 0.8106\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.6236 - accuracy: 0.8016\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6236118078231812, 0.801639974117279]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuawmnaI-Zau"
      },
      "source": [
        "la rete soffre di un grande problema di overfitting. inoltre i risultati sono scarsi rispetto a un semplice percettrone multistrato.\n",
        "le reti ricorrenti soffrono del problema della ** scomparsa del gradiente ** cosa che le rende inadatte per sequenze di dati troppo lunghe\n",
        "\n",
        "in questi casi la soluzione migliore sono le ** Long Short-term memory network **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FH5V6shK-LGk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dropout.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPzRCW8H8HTBj0N47oPwSQ1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zideric/colab/blob/main/Dropout.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edFpYovnQdgI"
      },
      "source": [
        "# Dropout\r\n",
        "\r\n",
        "questa tecnica di riduzione dell'overfitting consiste nell'escludere ad ogni esecuzione in modo casuale alcuni neuroni in modo da impedire che il modello memorizzi il set di training (detta in parole poverissime)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCFi0Ag_Q3oE"
      },
      "source": [
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "from keras.utils import to_categorical\r\n",
        "\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "\r\n",
        "from keras.callbacks import History\r\n",
        "\r\n",
        "from keras import optimizers\r\n",
        "\r\n",
        "from time import time"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xq5hlMv0RJOA"
      },
      "source": [
        "## Carichiamo il dataset \r\n",
        "\r\n",
        "In questo notebook addestreremo una rete neurale in grado di comprendere se una recensione relativa a un film è positiva o negativa. A questo scopo utilizzeremo l'IMDB Moview Review Dataset un dataset di 50.000 recensioni etichettate come positive o negative.\r\n",
        "Possiamo caricare il dataset utilizzando direttamente Keras, il parametro num_words ci serve per impostare il numero massimo di parole più frequenti da selezionare e di conseguenza il numero di features del nostro modello.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7jWF92zRFMA",
        "outputId": "bdd6e37b-06ed-4bf4-d736-18a0c4c6ceb0"
      },
      "source": [
        "from keras.datasets import imdb\r\n",
        "\r\n",
        "(X_train, y_train),(X_test, y_test) = imdb.load_data(num_words=5000)\r\n",
        "\r\n",
        "print(\"Numero di esempi nel train set: %d\" % len(X_train))\r\n",
        "print(\"Numero di esempi nel test set: %d\" % len(X_test))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Numero di esempi nel train set: 25000\n",
            "Numero di esempi nel test set: 25000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7r7w99vRnMH"
      },
      "source": [
        "Le recensioni sono già codificate in delle liste di id, un id identifica la posizione della relativa parola all'interno della lista ordinata delle parole che compaiono più spesso all'interno del nostro corpus di testo, con un offset di 3 posizioni. L'offset è utilizzato perché le prime 3 posizioni sono utilizzate per boh."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGcZOnuJRgX3",
        "outputId": "7db3dbb4-27a2-43cf-e8d8-3447d31c0f02"
      },
      "source": [
        "print(\"La prima recensione del train set contiene %d parole\" % len(X_train[0]))\r\n",
        "print(\"gli ID delle prime 10 parole della recensione: %s\" % X_train[0][:10])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "La prima recensione del train set contiene 218 parole\n",
            "gli ID delle prime 10 parole della recensione: [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpW20YJnR_a4"
      },
      "source": [
        "la seconda parola della prima recensione ha id 14, quindi corrisponde all'11 parola più frequente all'interno del corpus. Possiamo ottenere la posizione di una parola all'interno della lista delle parole più frequenti tramite il dizionario word_index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4JEp2LKR-Vg",
        "outputId": "519f6fd7-0859-4441-f9fc-152b1a0d2668"
      },
      "source": [
        "word_index = imdb.get_word_index()\r\n",
        "word_index['love']-3"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "113"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYvVX0hoSSaA"
      },
      "source": [
        "La parola 'love' è la 113esima parola più frequente all'interno del nostro corpus (nota l'offset di 3). Per poter ricostruire una frase partendo dagli IDs dobbiamo conoscere la relazione inversa, cioè a quale parola è associato un determinato id."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "iBPYlGXOSO4x",
        "outputId": "8c922451-57e4-496f-ce4c-d5ce7a3f9944"
      },
      "source": [
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\r\n",
        "reverse_word_index.get(113+3) #i primi tre sono riservati"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'love'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_U1UpkcTesD"
      },
      "source": [
        "\r\n",
        "Adesso possiamo utilizzare il dizionario con la relazione parola->id per ricostruire una frase, facciamolo con la prima frase all'interno del set di addestramento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSctMN1yTR4T",
        "outputId": "bb394d91-9817-4abc-cf8a-80486dc8e1b8"
      },
      "source": [
        "decoded_review = [reverse_word_index.get(i -3, '?') for i in X_train[0]]\r\n",
        "decoded_review = ' '.join(decoded_review)\r\n",
        "\r\n",
        "print(\"REVIEW: \"+ decoded_review)\r\n",
        "print(\"\\n\")\r\n",
        "print(\"SENTIMENT [1=Positive/0=Negative]: %d\" % y_train[0])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "REVIEW: ? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly ? was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little ? that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big ? for the whole film but these children are amazing and should be ? for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was ? with us all\n",
            "\n",
            "\n",
            "SENTIMENT [1=Positive/0=Negative]: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYGwxQFvUAYT"
      },
      "source": [
        "## Preprocessing \r\n",
        "Le frasi nel corpus di testo sono già codificate in numeri, ma hanno una lunghezza differente, per poterle utilizzare per addestrare la nostra rete neurale abbiamo bisogno di codificarle per ottenere un numero di features consistente. Abbiamo diverse tecniche per farlo, utilizziamo la più semplice: il one hot encoding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUfKQYTvTURS"
      },
      "source": [
        "def onehot_encoding(data, size):\r\n",
        "  onehot = np.zeros((len(data), size))\r\n",
        "  for i, d in enumerate(data):\r\n",
        "    onehot[i,d] = 1.\r\n",
        "  return onehot"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zC36bp-kVHtM"
      },
      "source": [
        "La funzione definita prende in input un corpus di testo e per ogni frase crea un numero di variabili di comodo pari al numero di parole totale del corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R48q6xSDUv69",
        "outputId": "631fe414-96d2-42b6-ad3b-94739dd3b5cb"
      },
      "source": [
        "X_train = onehot_encoding(X_train, 5000)\r\n",
        "X_test = onehot_encoding(X_test, 5000)\r\n",
        "\r\n",
        "X_train.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 5000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UdylG2tWkK3"
      },
      "source": [
        "Potremmo ottenere risultati migliori utilizzando tecniche più complesse ottimizzate per il natural language processing (leggasi words embedding) ma per il momento non complichiamoci troppo la vita e passiamo alla creazione del nostro modello.\r\n",
        "\r\n",
        "## Creaiamo la rete neurale\r\n",
        "\r\n",
        "Creiamo un modello di rete neurale profonda con ben 5 strati, 1 di input, 3 nascosti e 1 di output. Trattandosi di un problema di classificazione binaria (positiva/negativa) come funzione di attivazione dell'ultimo strato usiamo la sigmoide e come funzione di costo la binary crossentropy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ecoujvj7VToU"
      },
      "source": [
        "model = Sequential()\r\n",
        "model.add(Dense(512, activation='relu', input_shape=(5000,)))\r\n",
        "model.add(Dense(128,activation='relu'))\r\n",
        "model.add(Dense(32,activation='relu'))\r\n",
        "model.add(Dense(8,activation='relu'))\r\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5N3ReT10mDw"
      },
      "source": [
        "il nostro set di addestramento è rappresentato da una matrice spasra, ogni esempio ha 5000 colonne ma soltanto qualche dozzine di parole, proviamo quindi ad utilizzare la funzione di ottimizzazione adamax che in questi casi dovrebbe portare a risultati migliori"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APs-7vTKVcLV"
      },
      "source": [
        "model.compile(optimizer='adamax', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaFNZVaQ1b5i"
      },
      "source": [
        "avviamo l'addestarmento per soltanto 10 epoche"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CxuRA521WGZ",
        "outputId": "9cbad503-f273-4ec7-fd2a-47c86f929e1f"
      },
      "source": [
        "model.fit(X_train, y_train, epochs=10, batch_size=512)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "49/49 [==============================] - 4s 66ms/step - loss: 0.5356 - accuracy: 0.7106\n",
            "Epoch 2/10\n",
            "49/49 [==============================] - 3s 66ms/step - loss: 0.2284 - accuracy: 0.9138\n",
            "Epoch 3/10\n",
            "49/49 [==============================] - 3s 67ms/step - loss: 0.1730 - accuracy: 0.9402\n",
            "Epoch 4/10\n",
            "49/49 [==============================] - 3s 66ms/step - loss: 0.1268 - accuracy: 0.9624\n",
            "Epoch 5/10\n",
            "49/49 [==============================] - 3s 68ms/step - loss: 0.0826 - accuracy: 0.9804\n",
            "Epoch 6/10\n",
            "49/49 [==============================] - 3s 67ms/step - loss: 0.0508 - accuracy: 0.9916\n",
            "Epoch 7/10\n",
            "49/49 [==============================] - 3s 67ms/step - loss: 0.0283 - accuracy: 0.9967\n",
            "Epoch 8/10\n",
            "49/49 [==============================] - 3s 66ms/step - loss: 0.0183 - accuracy: 0.9982\n",
            "Epoch 9/10\n",
            "49/49 [==============================] - 3s 66ms/step - loss: 0.0147 - accuracy: 0.9983\n",
            "Epoch 10/10\n",
            "49/49 [==============================] - 3s 66ms/step - loss: 0.0119 - accuracy: 0.9986\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd88e6c2c90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgNsFmnt6-f7"
      },
      "source": [
        "i risultati ottenuti sono incredibili... si, c'è overfitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vioDVkEa1jBf",
        "outputId": "b98627c2-442b-42c7-8047-22f191d0f9c1"
      },
      "source": [
        "model.evaluate(X_test,y_test)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 3s 4ms/step - loss: 0.5420 - accuracy: 0.8705\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5420125722885132, 0.8705199956893921]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVlkEq9P7JmM"
      },
      "source": [
        "## Applicare il dropout\r\n",
        "\r\n",
        "Cerchiamo di contrastare l'overfitting utilizzando una combinazione di regolarizzazione L2 e Dropout. Possiamo utilizzare il Dropout con Keras aggiungendo un'instanza della classe Dropout come fosse un nuovo strato, questa classe necessità di un unico parametro che raprresenta la percentuale di nodi da mantenere attivi."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8yX_oXs7E7c",
        "outputId": "58280174-50b2-4334-a904-ff30316969c6"
      },
      "source": [
        "from keras.regularizers import l2\r\n",
        "from keras.layers import Dropout\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "model = Sequential()\r\n",
        "model.add(Dense(512, activation='relu', input_shape=(5000,), kernel_regularizer=l2(0.1)))\r\n",
        "model.add(Dropout(0.5))\r\n",
        "model.add(Dense(128,activation='relu', kernel_regularizer=l2(0.01)))\r\n",
        "model.add(Dropout(0.5))\r\n",
        "model.add(Dense(32,activation='relu',kernel_regularizer=l2(0.001)))\r\n",
        "model.add(Dropout(0.5))\r\n",
        "model.add(Dense(8,activation='relu', kernel_regularizer=l2(0.01)))\r\n",
        "model.add(Dropout(0.5))\r\n",
        "model.add(Dense(1, activation='sigmoid'))\r\n",
        "\r\n",
        "model.compile(optimizer='adamax', loss='binary_crossentropy', metrics=['accuracy'])\r\n",
        "model.fit(X_train, y_train, epochs=100, batch_size=512)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 45.7339 - accuracy: 0.5142\n",
            "Epoch 2/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 1.6237 - accuracy: 0.6224\n",
            "Epoch 3/100\n",
            "49/49 [==============================] - 4s 76ms/step - loss: 1.0079 - accuracy: 0.7192\n",
            "Epoch 4/100\n",
            "49/49 [==============================] - 4s 76ms/step - loss: 0.8217 - accuracy: 0.7736\n",
            "Epoch 5/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.7517 - accuracy: 0.7938\n",
            "Epoch 6/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.6929 - accuracy: 0.8200\n",
            "Epoch 7/100\n",
            "49/49 [==============================] - 4s 76ms/step - loss: 0.6618 - accuracy: 0.8312\n",
            "Epoch 8/100\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 0.6439 - accuracy: 0.8368\n",
            "Epoch 9/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.6248 - accuracy: 0.8412\n",
            "Epoch 10/100\n",
            "49/49 [==============================] - 4s 76ms/step - loss: 0.6076 - accuracy: 0.8495\n",
            "Epoch 11/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.6011 - accuracy: 0.8468\n",
            "Epoch 12/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.5885 - accuracy: 0.8505\n",
            "Epoch 13/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.5732 - accuracy: 0.8546\n",
            "Epoch 14/100\n",
            "49/49 [==============================] - 4s 76ms/step - loss: 0.5748 - accuracy: 0.8521\n",
            "Epoch 15/100\n",
            "49/49 [==============================] - 4s 76ms/step - loss: 0.5628 - accuracy: 0.8577\n",
            "Epoch 16/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.5567 - accuracy: 0.8571\n",
            "Epoch 17/100\n",
            "49/49 [==============================] - 4s 76ms/step - loss: 0.5531 - accuracy: 0.8547\n",
            "Epoch 18/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.5437 - accuracy: 0.8644\n",
            "Epoch 19/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.5464 - accuracy: 0.8603\n",
            "Epoch 20/100\n",
            "49/49 [==============================] - 4s 76ms/step - loss: 0.5385 - accuracy: 0.8603\n",
            "Epoch 21/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.5265 - accuracy: 0.8617\n",
            "Epoch 22/100\n",
            "49/49 [==============================] - 4s 76ms/step - loss: 0.5247 - accuracy: 0.8651\n",
            "Epoch 23/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.5229 - accuracy: 0.8653\n",
            "Epoch 24/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.5299 - accuracy: 0.8607\n",
            "Epoch 25/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.5235 - accuracy: 0.8645\n",
            "Epoch 26/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.5126 - accuracy: 0.8683\n",
            "Epoch 27/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.5164 - accuracy: 0.8648\n",
            "Epoch 28/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.5039 - accuracy: 0.8707\n",
            "Epoch 29/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.5050 - accuracy: 0.8681\n",
            "Epoch 30/100\n",
            "49/49 [==============================] - 4s 76ms/step - loss: 0.5110 - accuracy: 0.8602\n",
            "Epoch 31/100\n",
            "49/49 [==============================] - 4s 76ms/step - loss: 0.5133 - accuracy: 0.8644\n",
            "Epoch 32/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.4969 - accuracy: 0.8751\n",
            "Epoch 33/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.4996 - accuracy: 0.8697\n",
            "Epoch 34/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.5008 - accuracy: 0.8699\n",
            "Epoch 35/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.4947 - accuracy: 0.8726\n",
            "Epoch 36/100\n",
            "49/49 [==============================] - 4s 76ms/step - loss: 0.4966 - accuracy: 0.8697\n",
            "Epoch 37/100\n",
            "49/49 [==============================] - 4s 76ms/step - loss: 0.4959 - accuracy: 0.8728\n",
            "Epoch 38/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.4922 - accuracy: 0.8721\n",
            "Epoch 39/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.4820 - accuracy: 0.8774\n",
            "Epoch 40/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.4914 - accuracy: 0.8728\n",
            "Epoch 41/100\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 0.4791 - accuracy: 0.8778\n",
            "Epoch 42/100\n",
            "49/49 [==============================] - 4s 76ms/step - loss: 0.4799 - accuracy: 0.8797\n",
            "Epoch 43/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.4776 - accuracy: 0.8788\n",
            "Epoch 44/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.4806 - accuracy: 0.8821\n",
            "Epoch 45/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.4797 - accuracy: 0.8808\n",
            "Epoch 46/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.4789 - accuracy: 0.8768\n",
            "Epoch 47/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.4705 - accuracy: 0.8866\n",
            "Epoch 48/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.4786 - accuracy: 0.8791\n",
            "Epoch 49/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.4761 - accuracy: 0.8812\n",
            "Epoch 50/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.4792 - accuracy: 0.8755\n",
            "Epoch 51/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.4801 - accuracy: 0.8803\n",
            "Epoch 52/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.4675 - accuracy: 0.8849\n",
            "Epoch 53/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.4695 - accuracy: 0.8850\n",
            "Epoch 54/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.4645 - accuracy: 0.8847\n",
            "Epoch 55/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.4704 - accuracy: 0.8809\n",
            "Epoch 56/100\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 0.4673 - accuracy: 0.8864\n",
            "Epoch 57/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.4670 - accuracy: 0.8829\n",
            "Epoch 58/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.4566 - accuracy: 0.8913\n",
            "Epoch 59/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.4628 - accuracy: 0.8880\n",
            "Epoch 60/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.4592 - accuracy: 0.8895\n",
            "Epoch 61/100\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 0.4605 - accuracy: 0.8892\n",
            "Epoch 62/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.4667 - accuracy: 0.8857\n",
            "Epoch 63/100\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 0.4575 - accuracy: 0.8885\n",
            "Epoch 64/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.4627 - accuracy: 0.8895\n",
            "Epoch 65/100\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 0.4647 - accuracy: 0.8880\n",
            "Epoch 66/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.4617 - accuracy: 0.8882\n",
            "Epoch 67/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.4610 - accuracy: 0.8885\n",
            "Epoch 68/100\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 0.4583 - accuracy: 0.8917\n",
            "Epoch 69/100\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 0.4519 - accuracy: 0.8924\n",
            "Epoch 70/100\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 0.4524 - accuracy: 0.8971\n",
            "Epoch 71/100\n",
            "49/49 [==============================] - 4s 76ms/step - loss: 0.4544 - accuracy: 0.8935\n",
            "Epoch 72/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.4493 - accuracy: 0.8947\n",
            "Epoch 73/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.4517 - accuracy: 0.8951\n",
            "Epoch 74/100\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 0.4476 - accuracy: 0.8948\n",
            "Epoch 75/100\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 0.4532 - accuracy: 0.8934\n",
            "Epoch 76/100\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 0.4569 - accuracy: 0.8910\n",
            "Epoch 77/100\n",
            "49/49 [==============================] - 4s 79ms/step - loss: 0.4414 - accuracy: 0.9001\n",
            "Epoch 78/100\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 0.4434 - accuracy: 0.8996\n",
            "Epoch 79/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.4483 - accuracy: 0.8955\n",
            "Epoch 80/100\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 0.4496 - accuracy: 0.8951\n",
            "Epoch 81/100\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 0.4479 - accuracy: 0.8968\n",
            "Epoch 82/100\n",
            "49/49 [==============================] - 4s 79ms/step - loss: 0.4452 - accuracy: 0.8986\n",
            "Epoch 83/100\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 0.4405 - accuracy: 0.9009\n",
            "Epoch 84/100\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 0.4398 - accuracy: 0.9008\n",
            "Epoch 85/100\n",
            "49/49 [==============================] - 4s 79ms/step - loss: 0.4299 - accuracy: 0.9062\n",
            "Epoch 86/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.4347 - accuracy: 0.9014\n",
            "Epoch 87/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.4425 - accuracy: 0.9013\n",
            "Epoch 88/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.4489 - accuracy: 0.8975\n",
            "Epoch 89/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.4390 - accuracy: 0.8992\n",
            "Epoch 90/100\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 0.4354 - accuracy: 0.9050\n",
            "Epoch 91/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.4329 - accuracy: 0.9053\n",
            "Epoch 92/100\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 0.4308 - accuracy: 0.9055\n",
            "Epoch 93/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.4402 - accuracy: 0.8994\n",
            "Epoch 94/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.4325 - accuracy: 0.9048\n",
            "Epoch 95/100\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 0.4350 - accuracy: 0.9031\n",
            "Epoch 96/100\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 0.4412 - accuracy: 0.9040\n",
            "Epoch 97/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.4354 - accuracy: 0.9054\n",
            "Epoch 98/100\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 0.4350 - accuracy: 0.9045\n",
            "Epoch 99/100\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.4298 - accuracy: 0.9092\n",
            "Epoch 100/100\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 0.4303 - accuracy: 0.9049\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd88ea32410>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQbw8viGCD0r"
      },
      "source": [
        "ora il modello è riuscito a generalizzare meglio\r\n",
        "\r\n",
        "## Mettiamo all'opera la rete neurale\r\n",
        "\r\n",
        "la seguente funzione prende in input una recensione e la fa processare al modello"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Me1z1ofJCQuS"
      },
      "source": [
        "from re import sub\r\n",
        "\r\n",
        "def preprocess(review):\r\n",
        "  #rimuoviamo l'eventuale punteggiatura\r\n",
        "  reveiw = sub(r'[^\\w\\s]','',review)\r\n",
        "  #convertiamo tutto in minuscolo\r\n",
        "  review = review.lower()\r\n",
        "  #creiamo un array di parole\r\n",
        "  review = review.split(\" \")\r\n",
        "\r\n",
        "  #array vuoto per inserire gli id\r\n",
        "  review_array=[]\r\n",
        "\r\n",
        "  #Iteriamo lungo le parole della recensione\r\n",
        "  for word in review:\r\n",
        "    #proseguiamo se la parola si trova all'interno della lista di parole del corpus di addestramento\r\n",
        "    if word in word_index:\r\n",
        "      #estraiamo l'indice della parola\r\n",
        "      index = word_index[word]\r\n",
        "      #proseguiamo se l'indice è minore o uguale a 5000 (altrimenti non lo abbiamo consdierato nel modello)\r\n",
        "      if index < 5000:\r\n",
        "        review_array.append(word_index[word]+3)\r\n",
        "\r\n",
        "  # eseguiamo one hot encoding sulla lista di indici\r\n",
        "  review_array = onehot_encoding([review_array],5000)\r\n",
        "\r\n",
        "  return review_array\r\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDtQPHkEGjrg"
      },
      "source": [
        "Un valore maggiore dell'output della rete corrisponde ad una recensione maggiormente positiva, scriviamo una semplice funzione per interpretare l'output della rete come il sentiment della recensione."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIyiNcIZFn67"
      },
      "source": [
        "def prob_to_sentiment(prob):\r\n",
        "    \r\n",
        "    if(prob>0.9): return \"fantastica\"\r\n",
        "    elif(prob>0.75): return \"ottima\"\r\n",
        "    elif(prob>0.55): return \"buona\" \r\n",
        "    elif(prob>0.45): return \"neutrale\"\r\n",
        "    elif(prob>0.25): return \"negativa\"\r\n",
        "    elif(prob>0.1): return \"brutta\"\r\n",
        "    else: return \"pessima\""
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wi_44YHGnNZ",
        "outputId": "f73ac29d-4738-4f72-f4c1-a30315896f65"
      },
      "source": [
        "review = \"what a waste of time and cash.. the movie was pointless. with no flow. no questions answered. just a waste. I never review movies but had to share how bad this was..compared to part 1- 2- and 3.... i don't know what else to say other than how misleading the commercial is.. the commercial was cut and spliced with video and audio that didn't even match what happened in the movie... you have been warned. when the movie was over.. people actually Boo'd. hopefully people will spread the word, and save others from throwing their money away. i know die-hard fans will go and give it a shot, but will be disappointed as well. Sinister was better and actually made you jump quite a few times.\"\r\n",
        "\r\n",
        "x=preprocess(review)\r\n",
        "y=model.predict(x)[0]\r\n",
        "print(\"REVIEW: %s\" % review)\r\n",
        "print(\"\\n\")\r\n",
        "print(\"La recensione è %s [%.6f]\" % (prob_to_sentiment(y), y))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "REVIEW: what a waste of time and cash.. the movie was pointless. with no flow. no questions answered. just a waste. I never review movies but had to share how bad this was..compared to part 1- 2- and 3.... i don't know what else to say other than how misleading the commercial is.. the commercial was cut and spliced with video and audio that didn't even match what happened in the movie... you have been warned. when the movie was over.. people actually Boo'd. hopefully people will spread the word, and save others from throwing their money away. i know die-hard fans will go and give it a shot, but will be disappointed as well. Sinister was better and actually made you jump quite a few times.\n",
            "\n",
            "\n",
            "La recensione è pessima [0.057806]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVFapu66HOmB"
      },
      "source": [
        "proviamo con la recensione di avengers infinity war"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkRws656HEPE",
        "outputId": "31034184-840a-411d-f970-aa128e05ae2c"
      },
      "source": [
        "\r\n",
        "review = \"This movie will blow your mind and break your heart - and make you desperate to go back for more. Brave, brilliant and better than it has any right to be.\"\r\n",
        "x = preprocess(review)\r\n",
        "y = model.predict(x)\r\n",
        "\r\n",
        "print(\"REVIEW: %s\" % review)\r\n",
        "print(\"\\n\")\r\n",
        "print(\"La recesione è %s [%.6f]\" % (prob_to_sentiment(y), y))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "REVIEW: This movie will blow your mind and break your heart - and make you desperate to go back for more. Brave, brilliant and better than it has any right to be.\n",
            "\n",
            "\n",
            "La recesione è fantastica [0.936425]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AJTAJl-HSoB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}